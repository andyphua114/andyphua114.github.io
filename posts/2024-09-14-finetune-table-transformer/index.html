<!doctype html><html lang=en-us><head><title>How to Fine-Tune Table Transformer on Your Own Domain-Specific Data | Hello</title>
<meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Learn how to fine-tune Microsoft‚Äôs Table Transformer for more accurate table detection and structure recognition in your domain"><meta name=generator content="Hugo 0.129.0"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=G-8WWJE6P22E"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8WWJE6P22E")}</script></head><body><nav class=navigation><a href=/><span class=arrow>‚Üê</span>Home</a>
<a href=/posts>Archive</a>
<a href=/categories/portfolio>Portfolio</a>
<a href=/tags>Tags</a>
<a href=/about>About</a></nav><main class=main><section id=single><h1 class=title>How to Fine-Tune Table Transformer on Your Own Domain-Specific Data</h1><div class=tip><time datetime="2025-09-14 21:03:00 +0800 +08">Sep 14, 2025</time>
<span class=split>¬∑
</span><span>1456 words
</span><span class=split>¬∑
</span><span>7 minute read</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#pre-requisites-and-data-formats>Pre-requisites and Data Formats</a></li><li><a href=#general-notes-for-fine-tuning>General Notes for Fine-Tuning</a></li><li><a href=#evaluation>Evaluation</a></li><li><a href=#table-extraction>Table Extraction</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></details></aside><div class=content><p>The Table Transformer was introduced in <a href=https://arxiv.org/abs/2110.00061 target=_blank rel=noopener>&ldquo;PubTables-1M: Towards comprehensive table extraction from unstructured documents&rdquo;</a> by a set of authors from Microsoft. They trained two object detection models (based on the DETR architecture) with 947,642 fully annotated tables (PubTables-1M dataset) ‚Äî one for table detection and one for table structure recognition and released under this <a href=https://github.com/microsoft/table-transformer target=_blank rel=noopener>repo</a> with MIT license.</p><figure class=markdown-image><img src=/images/table-transformer.jpg alt="Microsoft Table Detection and Structure Recognition" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Microsoft Table Detection and Structure Recognition</p></figcaption></figure><p>While the Table Transformer Detection and Structure Recognition base models worked pretty well, I needed something that was more accurate and robust. There were limited resources available on how to fine tune the Table Transformer; the closest I could find was a <a href=https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_%28balloon%29.ipynb target=_blank rel=noopener>notebook by Niels Rogge</a> from the HuggingFace team who has a notebook on fine-tuning a DETR (DEtection TRansformer). He also has a couple of <a href=https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Table%20Transformer target=_blank rel=noopener>useful notebooks</a> on inference using Table Transformer. Credits to these resources as they form a very big basis of what I did here.</p><p>There were a couple of asks on HuggingFace discussions on how to fine-tune the Table Transformer with own domain-specific data, such as <a href=https://huggingface.co/microsoft/table-transformer-structure-recognition/discussions/1 target=_blank rel=noopener>this</a> and <a href=https://huggingface.co/microsoft/table-transformer-detection/discussions/16 target=_blank rel=noopener>this</a> as well as on the official Table Transformer <a href="https://github.com/microsoft/table-transformer/issues?q=fine+tune" target=_blank rel=noopener>github issues</a>, but there were no real guidance on how to go about doing it. I hope that this article and <a href=https://github.com/andyphua114/table-transformer-finetune-eval target=_blank rel=noopener>code repo</a> will help to plug the gap in this space.</p><h2 id=pre-requisites-and-data-formats>Pre-requisites and Data Formats <a href=#pre-requisites-and-data-formats class=anchor>üîó</a></h2><p>In the <a href=https://andyphua114.github.io/posts/2025-09-14-annotate-data/ target=_blank rel=noopener>first part of this series</a>, I wrote about how I set up a fully-local Label Studio for table annotation to get a clean domain-specific dataset ready for fine-tuning the table transformer.</p><p>There was a fair bit of confusion around correct bounding box format for DETR training as can be seen in <a href=https://github.com/huggingface/transformers/issues/32835 target=_blank rel=noopener>this github issue</a>. I will summarize the required format at various stages of the pipeline for a much clearer understanding.</p><p>To being with, here are a few common bounding boxes format:</p><ul><li>COCO format is xmin, ymin, width, height</li><li>YOLO format is xcenter, ycenter, width, height</li><li>Pascal VOC format is xmin, ymin, xmax, ymax</li></ul><p>In some cases, the bounding boxes can also be further normalized to the range [0,1].</p><p>I defined a custom <code>CocoDetection</code> class in <code>torchvision.datasets</code> which makes it easy to load COCO-style datasets in PyTorch. The addition is the <code>processor</code> to resize and normalize the images and to turn the annotations (from COCO format) to the format that DETR expects (which is normalized YOLO format).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CocoDetection</span>(torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>CocoDetection):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, img_folder, processor, train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        ann_file <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(
</span></span><span style=display:flex><span>            img_folder, <span style=color:#e6db74>&#34;custom_train.json&#34;</span> <span style=color:#66d9ef>if</span> train <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;custom_val.json&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        super(CocoDetection, self)<span style=color:#f92672>.</span>__init__(img_folder, ann_file)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>processor <span style=color:#f92672>=</span> processor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
</span></span><span style=display:flex><span>        <span style=color:#75715e># read in PIL image and target in COCO format</span>
</span></span><span style=display:flex><span>        img, target <span style=color:#f92672>=</span> super(CocoDetection, self)<span style=color:#f92672>.</span>__getitem__(idx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)</span>
</span></span><span style=display:flex><span>        image_id <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>ids[idx]
</span></span><span style=display:flex><span>        target <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;image_id&#34;</span>: image_id, <span style=color:#e6db74>&#34;annotations&#34;</span>: target}
</span></span><span style=display:flex><span>        encoding <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>processor(images<span style=color:#f92672>=</span>img, annotations<span style=color:#f92672>=</span>target, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>        pixel_values <span style=color:#f92672>=</span> encoding[<span style=color:#e6db74>&#34;pixel_values&#34;</span>]<span style=color:#f92672>.</span>squeeze()  <span style=color:#75715e># remove batch dimension</span>
</span></span><span style=display:flex><span>        target <span style=color:#f92672>=</span> encoding[<span style=color:#e6db74>&#34;labels&#34;</span>][<span style=color:#ae81ff>0</span>]  <span style=color:#75715e># remove batch dimension</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> pixel_values, target
</span></span></code></pre></div><p>For evaluation that is defined under the <code>compute_metrics</code> function, you would see that the bboxes of the targets (which is from the <code>val dataset</code>) are first converted using the <code>convert_bbox_yolo_to_pascal</code> function as the <code>val dataset</code> was loaded using the <code>CocoDetection</code> class. This means that when the bboxes of the targets are passed to the <code>compute_metrics</code> function, they are already in normalized YOLO format and hence there is a need to convert it to Pascal VOC format (which is the format used for metric evaluation)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>convert_bbox_yolo_to_pascal</span>(boxes, image_size):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            boxes (torch.Tensor): Bounding boxes in YOLO format
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            image_size (Tuple[int, int]): Image size in format (height, width)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># convert center to corners format</span>
</span></span><span style=display:flex><span>        boxes <span style=color:#f92672>=</span> center_to_corners_format(boxes)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># convert to absolute coordinates</span>
</span></span><span style=display:flex><span>        height, width <span style=color:#f92672>=</span> image_size
</span></span><span style=display:flex><span>        boxes <span style=color:#f92672>=</span> boxes <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>tensor([[width, height, width, height]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> boxes
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@torch.no_grad</span>()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_metrics</span>(
</span></span><span style=display:flex><span>        evaluation_results, image_processor, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>, id2label<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Compute mean average mAP, mAR and their variants for the object detection task.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            evaluation_results (EvalPrediction): Predictions and targets from evaluation.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            Mapping[str, float]: Metrics in a form of dictionary {&lt;metric_name&gt;: &lt;metric_value&gt;}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        predictions, targets <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>            evaluation_results<span style=color:#f92672>.</span>predictions,
</span></span><span style=display:flex><span>            evaluation_results<span style=color:#f92672>.</span>label_ids,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        image_sizes <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        post_processed_targets <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        post_processed_predictions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Process targets</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> targets:
</span></span><span style=display:flex><span>            batch_image_sizes <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(np<span style=color:#f92672>.</span>array([x[<span style=color:#e6db74>&#34;orig_size&#34;</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> batch]))
</span></span><span style=display:flex><span>            image_sizes<span style=color:#f92672>.</span>append(batch_image_sizes)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> image_target <span style=color:#f92672>in</span> batch:
</span></span><span style=display:flex><span>                boxes <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(image_target[<span style=color:#e6db74>&#34;boxes&#34;</span>])
</span></span><span style=display:flex><span>                boxes <span style=color:#f92672>=</span> convert_bbox_yolo_to_pascal(boxes, image_target[<span style=color:#e6db74>&#34;orig_size&#34;</span>])
</span></span><span style=display:flex><span>                labels <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(image_target[<span style=color:#e6db74>&#34;class_labels&#34;</span>])
</span></span><span style=display:flex><span>                post_processed_targets<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;boxes&#34;</span>: boxes, <span style=color:#e6db74>&#34;labels&#34;</span>: labels})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Process predictions</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> batch, target_sizes <span style=color:#f92672>in</span> zip(predictions, image_sizes):
</span></span><span style=display:flex><span>            batch_logits, batch_boxes <span style=color:#f92672>=</span> batch[<span style=color:#ae81ff>1</span>], batch[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>            output <span style=color:#f92672>=</span> ModelOutput(
</span></span><span style=display:flex><span>                logits<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>tensor(batch_logits), pred_boxes<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>tensor(batch_boxes)
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            post_processed_output <span style=color:#f92672>=</span> image_processor<span style=color:#f92672>.</span>post_process_object_detection(
</span></span><span style=display:flex><span>                output, threshold<span style=color:#f92672>=</span>threshold, target_sizes<span style=color:#f92672>=</span>target_sizes
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            post_processed_predictions<span style=color:#f92672>.</span>extend(post_processed_output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Compute metrics</span>
</span></span><span style=display:flex><span>        metric <span style=color:#f92672>=</span> MeanAveragePrecision(box_format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;xyxy&#34;</span>, class_metrics<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>For the bboxes of the predictions, the output from the Table Transformer model is in normalized YOLO format, and using the in-built processor method <code>post_process_object_detection</code>, it converts the normalized YOLO format to Pascal VOC format.</p><p>Hence when we perform the evaluation using <code>MeanAveragePrecision</code> from the <code>torchmetrics.detection.mean_ap</code>, we define the <code>box_format = "xyxy"</code> which is the Pascal VOC format.</p><h2 id=general-notes-for-fine-tuning>General Notes for Fine-Tuning <a href=#general-notes-for-fine-tuning class=anchor>üîó</a></h2><p>The other steps should be relatively straightforward if you have some experience with using HuggingFace <code>Trainer</code> class for fine-tuning Transformer models.</p><p>Of note is a custom <code>collate_fn</code> to batch images together. As the image processor resizes images to have a min/max size of 800 for the table detection model, images can have different sizes. We pad images (<code>pixel_values</code>) to the largest image in a batch, and create a corresponding <code>pixel_mask</code> to indicate which pixels are real (1) and which are padding (0). It is important to note the resizing for different models as I noticed that for <a href=https://huggingface.co/microsoft/table-transformer-detection/blob/main/preprocessor_config.json target=_blank rel=noopener>Table Transformer Detection</a>, both min and max (in some cases, renamed as shorted and longest edge) are 800 and 800, while for <a href=https://huggingface.co/microsoft/table-transformer-structure-recognition/blob/main/preprocessor_config.json target=_blank rel=noopener>Table Transformer Structure Recognition</a> they are 800 and 1,000 (it is also different for the <a href=https://huggingface.co/microsoft/table-transformer-structure-recognition-v1.1-all/blob/main/preprocessor_config.json target=_blank rel=noopener>v1.1-all model</a>). Check out the <code>preprocessor_config.json</code> to be clear of how input images should be preprocessed before being fed into the model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>collate_fn</span>(batch):
</span></span><span style=display:flex><span>    processor <span style=color:#f92672>=</span> AutoImageProcessor<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;microsoft/table-transformer-detection&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    pixel_values <span style=color:#f92672>=</span> [item[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> batch]
</span></span><span style=display:flex><span>    encoding <span style=color:#f92672>=</span> processor<span style=color:#f92672>.</span>pad(pixel_values, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> [item[<span style=color:#ae81ff>1</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> batch]
</span></span><span style=display:flex><span>    batch <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    batch[<span style=color:#e6db74>&#34;pixel_values&#34;</span>] <span style=color:#f92672>=</span> encoding[<span style=color:#e6db74>&#34;pixel_values&#34;</span>]
</span></span><span style=display:flex><span>    batch[<span style=color:#e6db74>&#34;pixel_mask&#34;</span>] <span style=color:#f92672>=</span> encoding[<span style=color:#e6db74>&#34;pixel_mask&#34;</span>]
</span></span><span style=display:flex><span>    batch[<span style=color:#e6db74>&#34;labels&#34;</span>] <span style=color:#f92672>=</span> labels
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> batch
</span></span></code></pre></div><p>The provided <a href=https://github.com/andyphua114/table-transformer-finetune-eval/blob/main/finetune.py target=_blank rel=noopener>code</a> comes with a set of hyperparameters to help you get started quickly, but you are strongly encouraged to perform your own hyperparameter tuning. However, these defaults should give you noticeable improvements in performance to give you a sanity check that everything is working properly before you invest further time and effort in hyperparameter tuning. I personally tried a simple hyperparameter search using Optuna and logging via wandb for experiment result visualization.</p><h2 id=evaluation>Evaluation <a href=#evaluation class=anchor>üîó</a></h2><p>In addition to training evaluation, you might also want to evaluate your fine-tuned models against other datasets. The <code>pycocotools</code>, as a fork of the original COCO API, provides a convenient way for evaluating and benchmarking object detection performance. I provided two codes for ease of evaluation. The <a href=https://github.com/andyphua114/table-transformer-finetune-eval/blob/main/model_inference.py target=_blank rel=noopener><code>model_inference.py</code></a> loads your fine tuned model for inference and save the predictions in a json format. The <a href=https://github.com/andyphua114/table-transformer-finetune-eval/blob/main/metric_evaluation.py target=_blank rel=noopener><code>metric_evaluation.py</code></a> uses the prediction saved as <a href=https://github.com/andyphua114/table-transformer-finetune-eval/blob/main/finetuned_pred.json target=_blank rel=noopener><code>finetune_pred.json</code></a> to evaluate against the ground truth defined in <a href=https://github.com/andyphua114/table-transformer-finetune-eval/blob/main/custom.json target=_blank rel=noopener><code>custom.json</code></a>. Below is an example of the metric evaluation results of my fine-tuned Table Transformer Detection model.</p><figure class=markdown-image><img src=/images/pycocotool_eval_example.jpg alt="pycocotools eval example" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Example of mAP from pycocotools evaluation</p></figcaption></figure><h2 id=table-extraction>Table Extraction <a href=#table-extraction class=anchor>üîó</a></h2><p>After fine-tuning the detection and structure recognition models, the last part of the pipeline is to place the tabular texts (using text extraction via OCR or directly from PDF) in the right cells.</p><p>I have found the use of <a href=https://github.com/conjuncts/gmft target=_blank rel=noopener><code>gmft</code> package</a> to be particularly useful. It is lightweight, modular, and performant, and relies on Table Transformers as the base models. This allows you to easily drop-in and replace the base models with your fine-tuned models.</p><p>Note that <code>gmft</code> focuses on tables and aims to maximize performance on tables alone. It is limited to extracting tables from digitized PDF documents and does not have in-built OCR support, although I don‚Äôt think it is too difficult to add on an OCR engine into the pipeline.</p><h2 id=conclusion>Conclusion <a href=#conclusion class=anchor>üîó</a></h2><p>When I first tried to fine-tune the Table Transformer, I couldn‚Äôt finda clear reference or working example. My hope is that this article helps in clarifying the main steps and gives you the confidence to adapt the code for your own use cases. Do feel free to let me know if you have any comments or questions.</p><h2 id=references>References <a href=#references class=anchor>üîó</a></h2><ul><li>Hugging Face Docs ‚Äî DETR: <a href=https://huggingface.co/docs/transformers/en/model_doc/detr target=_blank rel=noopener>https://huggingface.co/docs/transformers/en/model_doc/detr</a></li><li>Hugging Face Docs ‚Äî Table Transformer: <a href=https://huggingface.co/docs/transformers/en/model_doc/table-transformer target=_blank rel=noopener>https://huggingface.co/docs/transformers/en/model_doc/table-transformer</a></li><li>Table Transformer Paper (arXiv): <a href=https://arxiv.org/abs/2110.00061 target=_blank rel=noopener>https://arxiv.org/abs/2110.00061</a></li><li>Table Transformer GitHub Repository: <a href=https://github.com/microsoft/table-transformer target=_blank rel=noopener>https://github.com/microsoft/table-transformer</a></li><li>Tutorial ‚Äî Fine-tuning DETR on Custom Dataset (Balloon): <a href=https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR target=_blank rel=noopener>https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR</a> Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb</li></ul></div><div class=tags><a href=https://andyphua114.github.io/tags/python>python</a>
<a href=https://andyphua114.github.io/tags/fine-tuning>fine-tuning</a>
<a href=https://andyphua114.github.io/tags/transformers>transformers</a>
<a href=https://andyphua114.github.io/tags/microsoft-table-transformers>microsoft-table-transformers</a></div></section></main><footer id=footer><div class=copyright>¬© Copyright
2025
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg>
</span>Andy Phua</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-mini>nodejh</a></div></footer></body></html>