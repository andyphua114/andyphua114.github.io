<!doctype html><html lang=en-us><head><title>How to Annotate Tables in Label Studio for Table Transformer Fine-Tuning | Hello</title>
<meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Step-by-step guide to setting up Label Studio locally, configuring labeling setup for table detection and structure recognition, and exporting COCO datasets."><meta name=generator content="Hugo 0.129.0"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=G-8WWJE6P22E"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8WWJE6P22E")}</script></head><body><nav class=navigation><a href=/><span class=arrow>‚Üê</span>Home</a>
<a href=/posts>Archive</a>
<a href=/categories/portfolio>Portfolio</a>
<a href=/tags>Tags</a>
<a href=/about>About</a></nav><main class=main><section id=single><h1 class=title>How to Annotate Tables in Label Studio for Table Transformer Fine-Tuning</h1><div class=tip><time datetime="2025-09-14 20:41:00 +0800 +08">Sep 14, 2025</time>
<span class=split>¬∑
</span><span>1593 words
</span><span class=split>¬∑
</span><span>8 minute read</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#overview>Overview</a></li><li><a href=#getting-started>Getting Started</a></li><li><a href=#annotate-using-local-files>Annotate Using Local Files</a></li><li><a href=#labeling-config-for-table-detectionstructure-recognition>Labeling Config for Table Detection/Structure Recognition</a></li><li><a href=#semi-automate-labeling-tasks-with-machine-learning-backend>Semi-Automate Labeling Tasks with Machine Learning Backend</a></li><li><a href=#additional-tips-for-table-structure-labeling>Additional Tips for Table Structure Labeling</a></li><li><a href=#exporting-and-post-processing-labeled-data>Exporting and Post-Processing Labeled Data</a></li><li><a href=#possible-authentication-error>Possible Authentication Error</a></li><li><a href=#next-steps>Next Steps</a></li><li><a href=#references>References</a></li></ul></nav></div></details></aside><div class=content><p>Table extraction is not a one-size-fits-all problem. Off-the-shelf models often struggle with domain-specific documents, such as unusual layouts, merged cells, inconsistent formatting. For end to end table extraction, you first need to detect the tables within the document page (table detection). Next, you need to detect the table header, rows, columns to reconstruct the table cells (table structure recognition). Finally, place the tabular texts (using text extraction via OCR or directly from PDF) in the right cells.</p><p>When I first set out to fine-tune the Table Transformer model, I quickly realized that one of the biggest bottlenecks wasn‚Äôt the model itself. It was the data. Without properly annotated data, even the best model won‚Äôt perform well. That‚Äôs where high-quality annotation comes in.</p><p>In this article, I‚Äôll walk through how I set up a fully-local Label Studio for table annotation and how to make annotation manageable. By the end, you‚Äôll have a clean domain-specific dataset ready for fine-tuning the Table Transformer. If you already have a labeled dataset, you can check out the second part of this series for the Table Transformer fine-tuning guide.</p><h2 id=overview>Overview <a href=#overview class=anchor>üîó</a></h2><p><a href=https://github.com/HumanSignal/label-studio target=_blank rel=noopener>Label Studio</a> is an open source data labeling tool that lets you label data types like audio, text, images, videos, and time series with a simple and straightforward UI and export to various model formats. It is easy to install and setup by simply following the documentation. In this guide, I will cover some additional tips that are useful in my data annotation use case and specific to preparing for fine-tuning table transformer models.</p><h2 id=getting-started>Getting Started <a href=#getting-started class=anchor>üîó</a></h2><p>Firstly, if you are annotating PDF documents, you will first need to convert each page into an image. One nifty tool is <code>pdf2image</code> and you can refer to my <a href=https://github.com/andyphua114/label-studio/blob/main/image_converter.py target=_blank rel=noopener>helper script</a> (make sure poppler is installed as mentioned <a href=https://github.com/Belval/pdf2image target=_blank rel=noopener>here</a>).</p><p>Getting Label Studio installed and running is relatively straightforward. As usual, create a virtual environment (I highly recommend using <code>uv</code>). Then you just need to run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># remove &#34;uv&#34; if you are using the traditional pip. </span>
</span></span><span style=display:flex><span><span style=color:#75715e># In my case, I used version 1.17.0. You can use &#34;uv pip install label-studo==1.17.0&#34;</span>
</span></span><span style=display:flex><span>uv pip install label<span style=color:#f92672>-</span>studio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># start the local server</span>
</span></span><span style=display:flex><span>label<span style=color:#f92672>-</span>studio
</span></span></code></pre></div><p>A sign up is required even when running locally so as to create, manage, and collaborate on data labeling projects, especially for multi-user environments and production deployments, as it ties annotations to specific accounts and provides centralized project management.</p><h2 id=annotate-using-local-files>Annotate Using Local Files <a href=#annotate-using-local-files class=anchor>üîó</a></h2><p>Label Studio integrates with popular cloud and external storage systems, such as Amazon S3, Google Cloud Storage, Microsoft Azure Blob. However, if you want to work completely local and offline, you have the option of using your local storage and here‚Äôs how.</p><p>When creating your first project, you can skip the <code>Data Import</code> and <code>Labeling Setup</code> first if you would like to use your local files as your data for annotation. First, make sure you create the environment variables on your system <code>Edit environment variables for your account</code> and add the following:</p><figure class=markdown-image><img src=/images/env_vars.jpg alt="Environment variables setup" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Environment variables for using local files</p></figcaption></figure><p>The document root should be the root folder where your images are stored. In my case, my images are stored under <code>C:\Users\andyp\projects\label-studio\data</code> so my root is defined as <code>C:\Users\andyp\projects\label-studio</code>.</p><p>Once the project is created, go to <code>Settings</code> in the top right and select <code>Cloud Storage</code> on the sidebar. Click on <code>Add Source Storage</code>, fill in the required fields and click on <code>Check Connection</code>. If the status show as failed, make sure in the configuration you checked the button for <code>Treat every bucket object as a source file</code>. Once it is successful, click on <code>Add Storage</code> and then <code>Sync Storage</code>.</p><figure class=markdown-image><img src=/images/add_source_storage_validation_error.jpg alt="Validation error when adding source storage" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Validation error when adding source storage</p></figcaption></figure><figure class=markdown-image><img src=/images/add_source_storage_success.jpg alt="Adding source storage succesfully" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Adding source storage succesfully</p></figcaption></figure><h2 id=labeling-config-for-table-detectionstructure-recognition>Labeling Config for Table Detection/Structure Recognition <a href=#labeling-config-for-table-detectionstructure-recognition class=anchor>üîó</a></h2><p>Next, click on <code>Labeling Interface</code> to set up the labeling configuration. Click on <code>Browse Template</code> and select <code>Object Detection with Bounding Boxes</code>. Remove the template labels and add the labels for either table detection or table structure recognition.</p><p>Follow the <code>microsoft/table-transformer-detection</code> <a href=https://huggingface.co/microsoft/table-transformer-detection/blob/main/config.json target=_blank rel=noopener><code>config.json</code></a> <code>label2id</code> to setup the Labels. In my case, I do not have any rotated tables in my documents, hence I only add the <code>table</code> label.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#e6db74>&#34;label2id&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table rotated&#34;</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  }
</span></span></code></pre></div><figure class=markdown-image><img src=/images/labeling_interface_DET.jpg alt="Labeling interface for table detection" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Labeling interface for table detection</p></figcaption></figure><p>Similarly, follow the <code>microsoft/table-transformer-structure-recognition</code> <a href=https://huggingface.co/microsoft/table-transformer-structure-recognition/blob/main/config.json target=_blank rel=noopener><code>config.json</code></a> <code>label2id</code> to setup the Labels.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#e6db74>&#34;label2id&#34;</span><span style=color:#960050;background-color:#1e0010>:</span> {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table column&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table column header&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table projected row header&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table row&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;table spanning cell&#34;</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>  }
</span></span></code></pre></div><figure class=markdown-image><img src=/images/labeling_interface_STR.jpg alt="Labeling interface for table structure recognition" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Labeling interface for table structure recognition</p></figcaption></figure><p>Once the labeling interface is configured, you can start annotating your data.</p><figure class=markdown-image><img src=/images/annotate_after.jpg alt="Example of an annotated table" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Example of an annotated table</p></figcaption></figure><h2 id=semi-automate-labeling-tasks-with-machine-learning-backend>Semi-Automate Labeling Tasks with Machine Learning Backend <a href=#semi-automate-labeling-tasks-with-machine-learning-backend class=anchor>üîó</a></h2><p>To assist in data labeling, you can set up the Table Transformer base model as a machine learning (ML) backend for predictions. Instead of labeling from scratch, you can use the backend to predict the bounding boxes and adjust them before submitting as your annotation.</p><p>To get started, git clone the repo from Label Studio:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>git clone https:<span style=color:#f92672>//</span>github<span style=color:#f92672>.</span>com<span style=color:#f92672>/</span>HumanSignal<span style=color:#f92672>/</span>label<span style=color:#f92672>-</span>studio<span style=color:#f92672>-</span>ml<span style=color:#f92672>-</span>backend<span style=color:#f92672>.</span>git
</span></span></code></pre></div><p>Then create a virtual environment and install the requirements. If you face issues with any installation, install <code>label-studio-sdk</code> separately using <code>uv pip install label-studio-sdk</code> and comment out the <code>label-studio-sdk</code> in the <code>requirements.txt</code>. Thereafter install the rest of the packages using <code>uv pip install -r requirements.txt</code>.</p><p>Install the project repo using <code>uv pip install -e .</code> and now you can initialize and start creating your own ML backend using <code>label-studio-ml create table-transformer-backend</code>.</p><p>The <code>model.py</code> file is where you implement your own inference logic. You can refer to the <a href=https://github.com/andyphua114/label-studio-ml-backend/blob/main/table-transformer-backend/model.py target=_blank rel=noopener>file on my github</a> for a sample of a working ML table transformer backend.</p><p>Start the ML backend server using <code>label-studio-ml start table-transformer-backend</code>. By default it should be hosted on port 9090 and label studio will be able to connect to the backend via <code>127.0.0.1:9090</code>. If there is a validation error the first time, try <code>Validate and Save</code> again.</p><figure class=markdown-image><img src=/images/connect_ml_backend.jpg alt="Configuration to connect to ML backend" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Configuration to connect to ML backend</p></figcaption></figure><figure class=markdown-image><img src=/images/ml-backend-connected.jpg alt="Connected successfully" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Successful connection to ML backend</p></figcaption></figure><p>Once the ML backend is connected, it is straightforward to use it for prediction.</p><figure class=markdown-image><img src=/images/retrieve_predictions.jpg alt="Retrieving predictions from ML backend" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Retrieving predictions from ML backend</p></figcaption></figure><figure class=markdown-image><img src=/images/ml_prediction.jpg alt="Predictions from ML backend" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Predictions from ML backend</p></figcaption></figure><p>In my ML backend implementation, I am getting the ML backend to use the file that is stored locally, so there is no need to download and cache the image again. This is defined by the <a href=https://github.com/andyphua114/label-studio-ml-backend/blob/main/table-transformer-backend/model.py#L40 target=_blank rel=noopener>part of the code</a> <code>self.get_local_path</code> under the <code>predict</code> function. If you use the same code, you might be getting a similar error as below.</p><figure class=markdown-image><img src=/images/localpath_error.jpg alt="Error due to encoded URL path" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>Error due to encoded URL path</p></figcaption></figure><p>This error is due to the encoded URL path. To resolve this, go to the following package file in your venv: <code>label-studio-sdk</code> > <code>_extensions</code> > <code>label_studio_tools</code> > <code>core</code> > <code>io.py</code> and make the following amendments.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># add the unquote function from urllib.parse</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> urllib.parse <span style=color:#f92672>import</span> urlparse, unquote
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># look for the function get_local_path(), modify the function to below</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> is_local_storage_file:
</span></span><span style=display:flex><span>        filepath <span style=color:#f92672>=</span> url<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;?d=&#34;</span>)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        filepath <span style=color:#f92672>=</span> safe_build_path(LOCAL_FILES_DOCUMENT_ROOT, filepath)
</span></span><span style=display:flex><span>        filepath <span style=color:#f92672>=</span> unquote(filepath)  <span style=color:#75715e># add this code line</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(filepath):
</span></span><span style=display:flex><span>            logger<span style=color:#f92672>.</span>debug(
</span></span><span style=display:flex><span>                <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Local Storage file path exists locally, use it as a local file: </span><span style=color:#e6db74>{</span>filepath<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> filepath
</span></span></code></pre></div><p>The <code>unquote</code> is necessary as the file path obtained from the local storage file is URL-encoded (e.g space is encoded as <code>%20</code>), so there is a need to unquote to reformat to a proper file path.</p><h2 id=additional-tips-for-table-structure-labeling>Additional Tips for Table Structure Labeling <a href=#additional-tips-for-table-structure-labeling class=anchor>üîó</a></h2><p>For table structure labeling, you should ideally be working off the table image detected by the table detection model instead of the entire document page.</p><p>I highly recommend to first fine-tune the table transformer detection model so that it works highly accurately for your domain specific data. Thereafter, use the fine-tuned detection model to detect the table within your documents to extract and save only the table regions as images. You can refer to <a href=https://github.com/andyphua114/label-studio/blob/main/extract_table_image.py target=_blank rel=noopener>this code</a> for a simple workflow for inference and extraction using your fine-tuned detection model.</p><p>For the table structure labeling, follow the same steps as above to create a new project and use the table region images for your data labeling.</p><figure class=markdown-image><img src=/images/retrieve_predictions_STR.jpg alt="Predictions from Table Transformer Structure Recognition" width=800><figcaption style=font-size:13px;color:#5d5c5c;font-style:italic;text-align:center><p>The Table Transformer Structure Recognition base model is clearly lackluster</p></figcaption></figure><h2 id=exporting-and-post-processing-labeled-data>Exporting and Post-Processing Labeled Data <a href=#exporting-and-post-processing-labeled-data class=anchor>üîó</a></h2><p>After data labeling, export the dataset in COCO format. This is the format required for fine-tuning the Table Transformer. A more detailed explanation of the data format can be found in the <a href=https://andyphua114.github.io/posts/2025-09-14-finetune-table-transformer/ target=_blank rel=noopener>Part 2 article</a> of this series. Below is an example of how the COCO formatted dataset looks like.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;images&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;width&#34;</span>: <span style=color:#ae81ff>2550</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;height&#34;</span>: <span style=color:#ae81ff>3300</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;file_name&#34;</span>: <span style=color:#e6db74>&#34;images\\64cb5ad3__data%5Cmock-test_page_1.png&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;categories&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;table&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;annotations&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;image_id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;category_id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;segmentation&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;bbox&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>297.2336221043069</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>685.2974729900119</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1950.3027332161032</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1924.8955958885467</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;ignore&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;iscrowd&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;area&#34;</span>: <span style=color:#ae81ff>3754129.1418170724</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;info&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;year&#34;</span>: <span style=color:#ae81ff>2025</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;version&#34;</span>: <span style=color:#e6db74>&#34;1.0&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;description&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;contributor&#34;</span>: <span style=color:#e6db74>&#34;Label Studio&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;url&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;date_created&#34;</span>: <span style=color:#e6db74>&#34;2025-09-13 12:31:52.428644&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Since the <code>file_name</code> is based on the path in Label Studio, there is a need to replace the path to where the images are stored locally. You can use a <a href=https://github.com/andyphua114/label-studio/blob/main/rename_json.py target=_blank rel=noopener>simple helper script</a> for the path replacement.</p><h2 id=possible-authentication-error>Possible Authentication Error <a href=#possible-authentication-error class=anchor>üîó</a></h2><p>If you have implemented your own ML backend and is getting error <code>HTTPError: 401 Client Error:Unauthorized for url</code>, I suggest looking at this issue: <a href=https://github.com/HumanSignal/label-studio/issues/7519 target=_blank rel=noopener>https://github.com/HumanSignal/label-studio/issues/7519</a> and enabling the Legacy Token.</p><h2 id=next-steps>Next Steps <a href=#next-steps class=anchor>üîó</a></h2><p>With your labeled data, the next step is to perform fine-tuning of the Table Transformer base models which I covered <a href=https://andyphua114.github.io/posts/2025-09-14-finetune-table-transformer/ target=_blank rel=noopener>here</a>.</p><h2 id=references>References <a href=#references class=anchor>üîó</a></h2><ul><li>Label Studio Quick Start Guide: <a href=https://labelstud.io/guide/quick_start target=_blank rel=noopener>https://labelstud.io/guide/quick_start</a></li><li>Label Studio ML Backend Guide: <a href=https://labelstud.io/guide/ml_create.html target=_blank rel=noopener>https://labelstud.io/guide/ml_create.html</a></li><li>Hugging Face Docs ‚Äî DETR: <a href=https://huggingface.co/docs/transformers/en/model_doc/detr target=_blank rel=noopener>https://huggingface.co/docs/transformers/en/model_doc/detr</a></li><li>Hugging Face Docs ‚Äî Table Transformer: <a href=https://huggingface.co/docs/transformers/en/model_doc/table-transformer target=_blank rel=noopener>https://huggingface.co/docs/transformers/en/model_doc/table-transformer</a></li><li>Table Transformer Paper (arXiv): <a href=https://arxiv.org/abs/2110.00061 target=_blank rel=noopener>https://arxiv.org/abs/2110.00061</a></li><li>Table Transformer GitHub Repository: <a href=https://github.com/microsoft/table-transformer target=_blank rel=noopener>https://github.com/microsoft/table-transformer</a></li><li>Tutorial ‚Äî Fine-tuning DETR on Custom Dataset (Balloon): <a href=https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR target=_blank rel=noopener>https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR</a> Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb</li><li>uv (Fast Python Package Installer): <a href=https://github.com/astral-sh/uv target=_blank rel=noopener>https://github.com/astral-sh/uv</a></li></ul></div><div class=tags><a href=https://andyphua114.github.io/tags/label-studio>label-studio</a>
<a href=https://andyphua114.github.io/tags/python>python</a>
<a href=https://andyphua114.github.io/tags/transformers>transformers</a>
<a href=https://andyphua114.github.io/tags/microsoft-table-transformers>microsoft-table-transformers</a></div></section></main><footer id=footer><div class=copyright>¬© Copyright
2025
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg>
</span>Andy Phua</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-mini>nodejh</a></div></footer></body></html>